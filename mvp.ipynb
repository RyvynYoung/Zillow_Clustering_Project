{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zillow Clustering Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Finish the mvp workflow and then go back and think critically about what you might of missed. The point isn't to doubt yourself; the purpose here is to check your blind spots and see if you can't find more information or insights that will help you to deliver better results.\n",
    "\n",
    "Better yet, and this cannot be stressed enough, don't ask yourself these questions, but bring them up in conversation with peers, experts in other fields, or even complete strangers -- anyone with a different point of view is going to be able to help you to see what things you are taking for granted</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description: \n",
    "- Zillow: What is driving the errors in the Zestimates?\n",
    "- The presentation will consist of a notebook demo of the discoveries you made and work you have done related to uncovering what the drivers of the error in the zestimate is.\n",
    "\n",
    "\n",
    "## Problem Statement\n",
    "- What is driving the error in the Zestimate (logerror)?\n",
    "- Are there features that will produce a smaller average error?\n",
    "- What groups are in the data?\n",
    "- Does using these groups help remove the noise in the model?\n",
    "\n",
    "#### Domain Research\n",
    "- What is a single unit housing unit?\n",
    "    - https://www.investopedia.com/terms/h/housingunits.asp\n",
    "- What is fips?\n",
    "    - https://en.wikipedia.org/wiki/FIPS_county_code\n",
    "- What is the min/max tax rate by county in US?\n",
    "    - https://www.attomdata.com/news/market-trends/figuresfriday/top-10-u-s-counties-with-the-greatest-effective-tax-rates/\n",
    "- Understanding Zillow Zestimate\n",
    "    - https://www.zillow.com/zestimate/\n",
    "    - https://www.youtube.com/watch?v=rfWzMI_VwTQ\n",
    "    - https://www.kaggle.com/c/zillow-prize-1/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAN\n",
    "\n",
    "**MVP Questions to answer:**\n",
    "- Use clustering to identify new features\n",
    "- Determine the baseline logerror\n",
    "- Produce a model that has less logerror than baseline\n",
    "- Determine key drivers of error\n",
    "\n",
    "**Brainstorm:**    \n",
    "- MVP all counties\n",
    "- features to add:\n",
    "    - county name column, remove fips, get dummies for clustering use\n",
    "    - combined bedroom/bathroom ratio? drop bathroom, keep bedroom\n",
    "    - cluster on size with lot size, finished sq ft, and ????\n",
    "    - convert year build to age (current date - yearbuilt)\n",
    "    - does age correlate with size? if so maybe cluster age with size columns above\n",
    "    - cluster on tax value? taxamount, taxvaluedollarcnt, landtaxvaluedollarcnt, structuretaxvaluedollarcnt?\n",
    "- 2nd run add tax percent rate column and remove outliers above 6.6% and below 1% \n",
    "    - first outlier method removed too many observations, this was done instead\n",
    "    \n",
    "- 2nd run through seperate models by county?\n",
    "\n",
    "\n",
    "Audience: Zillow Data Science Team    \n",
    "Setting: Professional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire\n",
    "DELIVERABLES: \n",
    "- Data is collected from the Codeup cloud database with an appropriate SQL query\n",
    "- Define single unit property\n",
    "\n",
    "Summary of query requirements and creation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "Acquire    \n",
    "**take aways**\n",
    "1. could fireplace, garage, pool, hottub, deck be made to 0 or 1 then summed as \"plus_item\" column?\n",
    "    - this would assume null values do not have the feature, as opposed to feature is present but not noted\n",
    "2. drop features with 70% or more missing values to start\n",
    "3. most rows have 32-34 columns with missing values\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Prep needs additional summary on handling nulls and removing outliers</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrangle_zillow\n",
    "\n",
    "df, X_train, y_train, X_validate, y_validate, X_test, y_test, X_train_scaled, X_validate_scaled, X_test_scaled = wrangle_zillow.wrangle_zillow_cluster()\n",
    "# prints shape of X and X scaled dataframess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "DELIVERABLES:\n",
    "- Column data types are appropriate for the data they contain\n",
    "- Missing values are investigated and handled\n",
    "- Outliers are investigated and handled\n",
    "\n",
    "Summary of handling nulls, outliers, creating additional features, dropping columns/rows\n",
    "\n",
    "Data split into X (features) and y (target) dataframes for Train, Validate, and Test\n",
    "Data for X dataframes scaled for cluster exploration and modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During this stage called this summary function to assist in seeing remain nulls to handle and\n",
    "# for histograms of individual variables.\n",
    "# Can be used for full dataframe or any of the split or scaled dataframes\n",
    "import summarize\n",
    "\n",
    "fulldf = summarize.df_summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "Prepare     \n",
    "**take aways**    \n",
    "\n",
    "**This is deffinition used in previous project**   \n",
    "\n",
    "Determine deffinition of single property used article by James Chen Updated Sep 11, 2020 What Is a Housing Unit? \"The term housing unit refers to a single unit within a larger structure that can be used by an individual or household to eat, sleep, and live. The unit can be in any type of residence such as a house, apartment, mobile home, or may also be a single unit in a group of rooms. Essentially, a housing unit is deemed to be a separate living quarter where the occupants live and eat separately from other residents of the structure or building. They also have direct access from the building's exterior or through a common hallway.\"\n",
    "https://www.investopedia.com/terms/h/housingunits.asp   \n",
    "\n",
    "**In my opinion deffinition should include condo, townhouse, any unit that can be sold to an individual owner. So my deffinition will include townhouse, condo, ect. but not commercial, business, land only, etc. **   \n",
    "\n",
    "This site has the property use codes for LA county https://www.titleadvantage.com/mdocs/LA%20County%20Use%20Codes%20nm.pdf   \n",
    "looking at the common use codes for Duplex, Triplex, and Quadplex these codes indicate the units are multi-family/income properties or retail/store properties so these will be excluded\n",
    "\n",
    "Identify Properties in the Database: Based on the above definition some categories do not fit brief   \n",
    "Propertylandusetypeid | propertylandusedesc    \n",
    "No 31 Commercial/Office/Residential Mixed Used (not a residence)    \n",
    "No 46 Multi-Story Store (not a residence)   \n",
    "No 47 Store/Office (Mixed Use) (not a residence)    \n",
    "No 246 Duplex (2 Units, Any Combination)    \n",
    "No 247 Triplex (3 Units, Any Combination)   \n",
    "No 248 Quadruplex (4 Units, Any Combination)   \n",
    "260 Residential General    \n",
    "261 Single Family Residential   \n",
    "262 Rural Residence   \n",
    "263 Mobile Home   \n",
    "264 Townhouse   \n",
    "No 265 Cluster Home    \n",
    "266 Condominium    \n",
    "No 267 Cooperative (become shareholder not owner)   \n",
    "268 Row House       \n",
    "No 269 Planned Unit Development   \n",
    "No 270 Residential Common Area (propterty feature)    \n",
    "No 271 Timeshare (become shareholder not owner)    \n",
    "273 Bungalow      \n",
    "274 Zero Lot Line   \n",
    "275 Manufactured, Modular, Prefabricated Homes   \n",
    "276 Patio Home    \n",
    "279 Inferred Single Family Residential      \n",
    "No 290 Vacant Land - General (not a residence)   \n",
    "No 291 Residential Vacant Land (not a residence)   \n",
    "\n",
    "So we will keep only those where propertylandusetypeid = ('260', '261', '262', '263', '264', '266', '268', '273', '274', '275', '276', '279')  \n",
    "\n",
    "**acquire function updated to filter only for these**\n",
    "new shape = (71789, 70)\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Explore needs cleaned up and funcions created and called. Also need to add interaction between indenpendent varialbes and target. Add 4th clustering model on location?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "DELIVERABLES:      \n",
    "**- Interaction between independent variables and the target variable is explored using visualization and statistical testing**\n",
    "- Clustering is used to explore the data\n",
    "- A conclusion, supported by statistical testing and visualization, is drawn on whether or not the clusters are helpful/useful\n",
    "- At least 3 combinations of features for clustering should be tried\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster on size with lot size, finished sq ft, and ????\n",
    "does age correlate with size? if so maybe cluster age with size columns above\n",
    "cluster on tax value? taxamount, taxvaluedollarcnt, landtaxvaluedollarcnt, structuretaxvaluedollarcnt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does age correlate with size?\n",
    "1. decided to use calculatedfinishedsquarefeet for size to answer this\n",
    "2. visualize age and size together (both continuous variables)\n",
    "3. use scatter plot for chart\n",
    "4. use stats.pearsonr for hypothesis testing\n",
    "\n",
    "#### Hypothesis test\n",
    "Ho: There is no relationship between age and size.     \n",
    "Ha: There is a relationship between age and size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for final this will need to be a function\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.scatterplot(data=X_train, y='calculatedfinishedsquarefeet', x='age', hue='county', alpha=.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this visual it may be better to separate by county and retest in next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for final this will need to be a function\n",
    "alpha = .05\n",
    "x = X_train.age\n",
    "y = X_train.calculatedfinishedsquarefeet\n",
    "r, p = stats.pearsonr(x, y)\n",
    "r, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is less than alpha so we reject the null hypothesis, though the r value is not far from zero and is negative, which would indicate a weak negative linear correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on correlation of age and size (though weak) will try clustering on calculatedfinishedsquarefeet, lotsizesquarefeet, and age.    \n",
    "This produced a very slight improvement over baseline. Removing age from this cluster to see impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_vars = ['calculatedfinishedsquarefeet_scaled', 'lotsizesquarefeet_scaled']\n",
    "explore.elbow_plot(X_train_scaled, cluster_vars)\n",
    "# based on this will use 5 as k value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "cluster_col_name = 'size_cluster'\n",
    "train_clusters, kmeans = explore.run_kmeans(X_train_scaled, X_train, cluster_vars, k, cluster_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clusters.size_cluster.value_counts()\n",
    "# so so dispersal of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = explore.get_centroids(kmeans, cluster_vars, cluster_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train_scaled = explore.add_to_train(X_train, train_clusters, X_train_scaled, centroids, cluster_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_trans_clusters = explore.kmeans_transform(X_validate_scaled, kmeans, cluster_vars, cluster_col_name)\n",
    "test_trans_clusters = explore.kmeans_transform(X_test_scaled, kmeans, cluster_vars, cluster_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate, X_validate_scaled = explore.add_to_train(X_validate, val_trans_clusters, X_validate_scaled, centroids, cluster_col_name)\n",
    "X_test, X_test_scaled = explore.add_to_train(X_test, test_trans_clusters, X_test_scaled, centroids, cluster_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dollar amount clustering\n",
    "# use structuretaxvaluedollarcnt, taxvaluedollarcnt, taxamount, structure_dollar_per_sqft, land_dollar_per_sqft, taxrate\n",
    "cluster_vars2 = ['structuretaxvaluedollarcnt_scaled', 'taxvaluedollarcnt_scaled', 'taxamount_scaled', 'structure_dollar_per_sqft_scaled', 'land_dollar_per_sqft_scaled', 'taxrate_scaled']\n",
    "explore.elbow_plot(X_train_scaled, cluster_vars2)\n",
    "# based on this will use 5 as k value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2=5\n",
    "cluster_col_name2 = 'dollar_cluster'\n",
    "train_clusters2, kmeans2 = explore.run_kmeans(X_train_scaled, X_train, cluster_vars2, k2, cluster_col_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clusters2.dollar_cluster.value_counts()\n",
    "# based on this distribution this clustering may not be that helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids2 = explore.get_centroids(kmeans2, cluster_vars2, cluster_col_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train_scaled = explore.add_to_train(X_train, train_clusters2, X_train_scaled, centroids2, cluster_col_name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does bed/bath ratio correlate with age?\n",
    "1. visualize bed_bath_ratio and age together (both continuous variables)\n",
    "2. use scatter plot for chart\n",
    "3. based on chart bin bed_bath ratio and age and visualize\n",
    "4. use chi squared test to compare \n",
    "\n",
    "#### Hypothesis test\n",
    "Ho: Bed_bath_ratio and age are independent.     \n",
    "Ha: Bed_bath_ratio and age are dependent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for final this will need to be a function\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.scatterplot(data=X_train, y='age', x='bed_bath_ratio', hue='county', alpha=.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binned bed_bath_ratio column\n",
    "X_train['bb_bin'] = pd.cut(X_train.bed_bath_ratio, bins=[0, 1, 2, 3, 4], labels=['1', '2', '3', '4+'])\n",
    "# create binned age column by 20 yr groups\n",
    "X_train['age_bin'] = pd.cut(X_train.age, bins=[0, 20, 40, 60, 80], labels=['<20', '40', '60', '80+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize these 2 categorical variables together\n",
    "# Heatmap of bined values (Categorical and Categorical)\n",
    "plt.figure(figsize=(16,9))\n",
    "ctab = pd.crosstab(X_train.bb_bin, X_train.age_bin, normalize=True)\n",
    "sns.heatmap(ctab, cmap='Purples', annot=True, fmt='.1%')\n",
    "plt.title('What is correlation between bined bed/bath ratio and binned age of home?')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Test\n",
    "\n",
    "alpha = 0.05\n",
    "contingency_table = pd.crosstab(X_train.bb_bin, X_train.age_bin)\n",
    "\n",
    "chi2, p, degf, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis\")\n",
    "    print(f'chi^2 = {chi2:.4f}')\n",
    "    print(f'p     = {p:.4f}')\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rooms cluster\n",
    "# use bedroomcnt, fullbathcnt, roomcnt, structure_dollar_per_sqft, land_dollar_per_sqft, taxrate\n",
    "cluster_vars3 = ['bedroomcnt_scaled', 'fullbathcnt_scaled', 'roomcnt_scaled', 'bed_bath_ratio_scaled']\n",
    "explore.elbow_plot(X_train_scaled, cluster_vars3)\n",
    "# based on this will use 6 as k value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k3=6\n",
    "cluster_col_name3 = 'rooms_cluster'\n",
    "train_clusters3, kmeans3 = explore.run_kmeans(X_train_scaled, X_train, cluster_vars3, k3, cluster_col_name3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clusters3.rooms_cluster.value_counts()\n",
    "# pretty good dispersal of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids3 = explore.get_centroids(kmeans3, cluster_vars3, cluster_col_name3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train_scaled = explore.add_to_train(X_train, train_clusters3, X_train_scaled, centroids3, cluster_col_name3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd iteration: add area/location cluster with longitude, latitude, encoded counties?\n",
    "X_train_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create location cluster\n",
    "# use longitude, latitude, age\n",
    "cluster_vars4 = ['longitude_scaled', 'latitude_scaled', 'age_scaled']\n",
    "explore.elbow_plot(X_train_scaled, cluster_vars4)\n",
    "# based on this will use 6 as k value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k4=6\n",
    "cluster_col_name4 = 'loc_cluster'\n",
    "train_clusters4, kmeans4 = explore.run_kmeans(X_train_scaled, X_train, cluster_vars4, k4, cluster_col_name4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clusters4.loc_cluster.value_counts()\n",
    "# good dispersal of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids4 = explore.get_centroids(kmeans4, cluster_vars4, cluster_col_name4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train_scaled = explore.add_to_train(X_train, train_clusters4, X_train_scaled, centroids4, cluster_col_name4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Modeling needs various X_train dataframe creation moved to model.py and then try different types of models?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Stage\n",
    "DELIVERABLES: \n",
    "- At least 4 different models are created and their performance is compared\n",
    "- One model is the distinct combination of algorithm, hyperparameters, and features\n",
    "- Best practices on data splitting are followed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import model\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.feature_selection import f_regression, SelectKBest, RFE \n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from math import sqrt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a lineat regression model with original (not clustered) features\n",
    "XtrainSO = X_train_scaled.drop(columns=['size_cluster',\n",
    "       'centroid_calculatedfinishedsquarefeet_scaled',\n",
    "       'centroid_lotsizesquarefeet_scaled', 'dollar_cluster',\n",
    "       'centroid_structuretaxvaluedollarcnt_scaled',\n",
    "       'centroid_taxvaluedollarcnt_scaled', 'centroid_taxamount_scaled',\n",
    "       'centroid_structure_dollar_per_sqft_scaled',\n",
    "       'centroid_land_dollar_per_sqft_scaled', 'centroid_taxrate_scaled',\n",
    "       'rooms_cluster', 'centroid_bedroomcnt_scaled',\n",
    "       'centroid_fullbathcnt_scaled', 'centroid_roomcnt_scaled',\n",
    "       'centroid_bed_bath_ratio_scaled', 'loc_cluster',\n",
    "       'centroid_longitude_scaled', 'centroid_latitude_scaled',\n",
    "       'centroid_age_scaled'])\n",
    "XtrainSO.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a lineat regression model with original and size cluster features\n",
    "XtrainSS = X_train_scaled.drop(columns=['dollar_cluster',\n",
    "       'centroid_structuretaxvaluedollarcnt_scaled',\n",
    "       'centroid_taxvaluedollarcnt_scaled', 'centroid_taxamount_scaled',\n",
    "       'centroid_structure_dollar_per_sqft_scaled',\n",
    "       'centroid_land_dollar_per_sqft_scaled', 'centroid_taxrate_scaled',\n",
    "       'rooms_cluster', 'centroid_bedroomcnt_scaled',\n",
    "       'centroid_fullbathcnt_scaled', 'centroid_roomcnt_scaled',\n",
    "       'centroid_bed_bath_ratio_scaled', 'loc_cluster',\n",
    "       'centroid_longitude_scaled', 'centroid_latitude_scaled',\n",
    "       'centroid_age_scaled'])\n",
    "XtrainSS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a lineat regression model with original and dollar cluster features\n",
    "XtrainSD = X_train_scaled.drop(columns=['size_cluster',\n",
    "       'centroid_calculatedfinishedsquarefeet_scaled',\n",
    "       'centroid_lotsizesquarefeet_scaled', 'rooms_cluster', 'centroid_bedroomcnt_scaled',\n",
    "       'centroid_fullbathcnt_scaled', 'centroid_roomcnt_scaled',\n",
    "       'centroid_bed_bath_ratio_scaled', 'loc_cluster',\n",
    "       'centroid_longitude_scaled', 'centroid_latitude_scaled',\n",
    "       'centroid_age_scaled'])\n",
    "XtrainSD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a lineat regression model with original and room cluster features\n",
    "XtrainSR = X_train_scaled.drop(columns=['size_cluster',\n",
    "       'centroid_calculatedfinishedsquarefeet_scaled',\n",
    "       'centroid_lotsizesquarefeet_scaled', 'dollar_cluster',\n",
    "       'centroid_structuretaxvaluedollarcnt_scaled',\n",
    "       'centroid_taxvaluedollarcnt_scaled', 'centroid_taxamount_scaled',\n",
    "       'centroid_structure_dollar_per_sqft_scaled',\n",
    "       'centroid_land_dollar_per_sqft_scaled', 'centroid_taxrate_scaled',\n",
    "       'loc_cluster','centroid_longitude_scaled', 'centroid_latitude_scaled',\n",
    "       'centroid_age_scaled'])\n",
    "XtrainSR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a lineat regression model with original and location cluster features\n",
    "XtrainSL = X_train_scaled.drop(columns=['size_cluster',\n",
    "       'centroid_calculatedfinishedsquarefeet_scaled',\n",
    "       'centroid_lotsizesquarefeet_scaled', 'dollar_cluster',\n",
    "       'centroid_structuretaxvaluedollarcnt_scaled',\n",
    "       'centroid_taxvaluedollarcnt_scaled', 'centroid_taxamount_scaled',\n",
    "       'centroid_structure_dollar_per_sqft_scaled',\n",
    "       'centroid_land_dollar_per_sqft_scaled', 'centroid_taxrate_scaled',\n",
    "       'rooms_cluster', 'centroid_bedroomcnt_scaled',\n",
    "       'centroid_fullbathcnt_scaled', 'centroid_roomcnt_scaled',\n",
    "       'centroid_bed_bath_ratio_scaled'])\n",
    "XtrainSL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now a model with all features and clusters\n",
    "XtrainSALL = X_train_scaled.copy()\n",
    "XtrainSALL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a model with only clusters\n",
    "XtrainSOC = X_train_scaled.drop(columns=['bedroomcnt_scaled', 'calculatedfinishedsquarefeet_scaled',\n",
    "       'fullbathcnt_scaled', 'lotsizesquarefeet_scaled', 'roomcnt_scaled',\n",
    "       'unitcnt_scaled', 'structuretaxvaluedollarcnt_scaled',\n",
    "       'taxvaluedollarcnt_scaled', 'taxamount_scaled', 'age_scaled',\n",
    "       'taxrate_scaled', 'structure_dollar_per_sqft_scaled',\n",
    "       'land_dollar_per_sqft_scaled', 'bed_bath_ratio_scaled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models\n",
    "***for MVP these are variations on Linear Regression Model altering features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = model.get_baseline(y_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled, original features only\n",
    "RMSE_m1SO = model.linear_reg_model(XtrainSO, y_train)\n",
    "RMSE_m1SO\n",
    "# slightly better than baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled, original with size cluster only\n",
    "RMSE_m2SS = model.linear_reg_model(XtrainSS, y_train)\n",
    "RMSE_m2SS\n",
    "# slightly better than original features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled, original with dollar cluster only\n",
    "RMSE_m3SD = model.linear_reg_model(XtrainSD, y_train)\n",
    "RMSE_m3SD\n",
    "# slightly better than size cluster 2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled, original with room cluster only\n",
    "RMSE_m4SR = model.linear_reg_model(XtrainSR, y_train)\n",
    "RMSE_m4SR\n",
    "# about the same as dollar cluster, 3rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled, original with location cluster only\n",
    "RMSE_m5SL = model.linear_reg_model(XtrainSL, y_train)\n",
    "RMSE_m5SL\n",
    "# about the same as size cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled, original with ALL clusters\n",
    "RMSE_m6SALL = model.linear_reg_model(XtrainSALL, y_train)\n",
    "RMSE_m6SALL\n",
    "# best of these, 1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled, only clusters\n",
    "RMSE_m7SOC = model.linear_reg_model(XtrainSOC, y_train)\n",
    "RMSE_m7SOC\n",
    "# barely better than baseline and worse than with no clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******\n",
    "**Take aways from Linear Regression models**     \n",
    "Model with all original features and clusters performed best. Though not much better than baseline.\n",
    "\n",
    "**Next Step**     \n",
    "- try alternate algorithm\n",
    "- create loop that will summarize and display results\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = [XtrainSO, XtrainSS, XtrainSD, XtrainSR, XtrainSL, XtrainSALL, XtrainSOC]\n",
    "target = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this doesn't work\n",
    "# for df in df2test:\n",
    "#     RMSE = model.lasso_lars(df, target)\n",
    "#     print(RMSE)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtrainSO.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_lars(x_scaleddf, target):\n",
    "    # Make a model\n",
    "    lars = LassoLars(alpha=1)\n",
    "    # Fit a model\n",
    "    lars.fit(x_scaleddf, target)\n",
    "    # Make Predictions\n",
    "    lars_pred = lars.predict(x_scaleddf)\n",
    "    # Computer root mean squared error\n",
    "    lars_rmse = sqrt(mean_squared_error(target, lars_pred))\n",
    "    return lars_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_m1SO = lasso_lars(XtrainSO, y_train)\n",
    "RMSE_m1SO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtrainSS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_m2SS = lasso_lars(XtrainSS, y_train)\n",
    "RMSE_m2SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_m3SALL = lasso_lars(XtrainSO, y_train)\n",
    "RMSE_m3SALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delivery Stage\n",
    "DELIVERABLES:\n",
    "- [Main_Notebook](xxxxxxx)\n",
    "- walk through of notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of findings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top perfoming model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes: place in correct section or remove?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determine and handle upper and lower outliers    \n",
    "While this area of the country does have a broad range of housing values and therefore tax values, we know from previous work with this dataset that the tax rate range is from .001% up to 45% which is unrealistic for tax rates in 2017. Therefore we will determine a point at which to drop outliers above and below the IQR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "MVP - used taxamount to determine cutoff for outliers    \n",
    "for 2nd round might add calculated tax rate column and use that for cut off instead.  \n",
    "**Upperbound Outliers to HANDLE**   \n",
    "\n",
    "**taxamount_outliers**     \n",
    "count      4894.000000     \n",
    "mean      10979.609790     \n",
    "std       20235.975915     \n",
    "min           0.830000     \n",
    "25%        1997.677500     \n",
    "50%        5367.625000     \n",
    "75%       12289.405000     \n",
    "max      573508.600000     \n",
    "Name: taxamount_outliers, dtype: float64     \n",
    "    \n",
    "*******\n",
    "**based on the above info drop upperbound outliers that are above 50th percentile of upperbound outliers**\n",
    "\n",
    "reasoning: there is a significant jump from the 25th to 50th percentile making this a logical place to start removing outliers while still keeping some outliers above the upperbound\n",
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********\n",
    "**Lowerbound Outliers to HANDLE**\n",
    "\n",
    "**taxamount_outliers**     \n",
    "count    68981.000000    \n",
    "mean      8203.408061    \n",
    "std       3350.880803    \n",
    "min       3210.485000    \n",
    "25%       5817.075000    \n",
    "50%       7473.745000    \n",
    "75%       9695.125000    \n",
    "max      21685.205000    \n",
    "Name: taxamount_low_outliers, dtype: float64                  \n",
    "   \n",
    "*******\n",
    "**based on the above drop rows that are above the 75th lowerbound percentile**\n",
    "\n",
    "reasoning: there is a significant jump from the 50th to 75th percentile making this a logical place to start removing outliers while still keeping some outliers above the lowerbound\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
